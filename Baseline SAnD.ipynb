{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jakqV3Scdyle"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "jakqV3Scdyle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sladGIRtTaeg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "id": "sladGIRtTaeg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5_clnk7T7yV"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "I5_clnk7T7yV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9zgM6LTYIc1",
        "outputId": "41a4de78-04b1-4872-a7ec-9ed2b680895a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/bitnet')\n"
      ],
      "id": "T9zgM6LTYIc1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JRzz_3FJYfz",
        "outputId": "d30390e7-4a4b-4b78-80cd-4384ecd7135c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if __name__=='__main__':\n",
        "    print('Using device:', device)"
      ],
      "id": "3JRzz_3FJYfz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMu3ZJFXY8Q8"
      },
      "outputs": [],
      "source": [
        "# !cat '/content/gdrive/My Drive/bitnet/processed/patients_mimic3_full.json'"
      ],
      "id": "wMu3ZJFXY8Q8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9da94921"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import data_proc\n",
        "from data_proc import Dataset"
      ],
      "id": "9da94921"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa4fb05",
        "outputId": "a1d6cd4e-f70c-4213-8521-cc6b98fa5d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of second level category:  170\n",
            "Length of reverse dictionary  3875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7496"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset = Dataset()\n",
        "data = dataset.load_data()\n",
        "data = data\n",
        "len(data)"
      ],
      "id": "8fa4fb05"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86429275",
        "outputId": "961052d0-d682-466f-d491-122538407b44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39, 3875, 170)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset.max_len_visit, dataset.vocabulary_size, dataset.digit3_size"
      ],
      "id": "86429275"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzLIHuH2LKLe",
        "outputId": "79c94a22-939e-401d-b93f-db5c06724755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 31, 56, 61, 90, 105, 109, 114, 146]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(data[0][1])\n",
        "print(data[0][4])"
      ],
      "id": "VzLIHuH2LKLe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuzba-RCk_3o",
        "outputId": "4c2ecfea-7f93-496e-dc0c-1b22c7ae502b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_codes 3434\n"
          ]
        }
      ],
      "source": [
        "pids = [i[0] for i in data]\n",
        "intervals = [i[1] for i in data]\n",
        "seqs = [i[2] for i in data]\n",
        "\n",
        "\n",
        "readmission = [i[4] for i in data]\n",
        "diag = [i[3] for i in data]\n",
        "\n",
        "num_codes = set([code for visits in seqs for visit in visits for code in visit])\n",
        "num_codes = len(set(num_codes)) \n",
        "\n",
        "print(\"num_codes\",num_codes)\n",
        "\n",
        "\n",
        "assert len(pids) == len(seqs) == len(intervals) == len(readmission)"
      ],
      "id": "fuzba-RCk_3o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSv5V_CKlOQt",
        "outputId": "e159629a-b5f9-4ec9-db33-531866baea4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7496\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, seqs, intervals, readmission, diag):\n",
        "        self.seqs = seqs\n",
        "        self.intervals = intervals\n",
        "        self.y1 = readmission\n",
        "        self.y2 = diag\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.y1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return self.seqs[index], self.intervals[index], self.y1[index], self.y2[index]\n",
        "data = CustomDataset(seqs, intervals, readmission, diag)\n",
        "print(len(data))"
      ],
      "id": "uSv5V_CKlOQt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyKvUqFqlb5U",
        "outputId": "6f78a428-0787-41ae-b4e1-3e57ae5d4563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 6003\n",
            "Length of val dataset: 743\n",
            "Length of test dataset: 750\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_test_split = int(len(data)*0.9)\n",
        "lengths = [train_test_split, len(data) - train_test_split]\n",
        "train_data, test_data = random_split(data, lengths)\n",
        "\n",
        "\n",
        "train_val_split = int(len(train_data)*0.89)\n",
        "lengths = [train_val_split, len(train_data) - train_val_split]\n",
        "train_data, val_data = random_split(train_data, lengths)\n",
        "\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_data))\n",
        "print(\"Length of val dataset:\", len(val_data))\n",
        "print(\"Length of test dataset:\", len(test_data))\n"
      ],
      "id": "FyKvUqFqlb5U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKmgHLO_lzTm"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "  sequences, intervals, labels1, labels2 = zip(*data)\n",
        "\n",
        "  num_patients = len(sequences)\n",
        "  num_visits = len(sequences[0])\n",
        "  num_codes = len(sequences[0][0])\n",
        "\n",
        "  y1 = torch.tensor(labels1, dtype=torch.float)\n",
        "  y2 = torch.tensor(labels2, dtype=torch.float)\n",
        "\n",
        "  return sequences, intervals, y1, y2"
      ],
      "id": "EKmgHLO_lzTm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N-gtAA4l2s8",
        "outputId": "cb5680a4-d7bf-4068-a911-080187fe873a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3434\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "\n",
        "def load_data(train_data, val_data, test_data, collate_fn):\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    train_loader = DataLoader(dataset = train_data, batch_size = 32, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(dataset = val_data, batch_size = 32, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(dataset = test_data, batch_size = 32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader = load_data(train_data, val_data, test_data, collate_fn)\n",
        "\n",
        "print(num_codes)"
      ],
      "id": "3N-gtAA4l2s8"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        for pos in range(seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1)) / d_model)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        seq_len = x.shape[1]\n",
        "        x = math.sqrt(self.d_model) * x\n",
        "        x = x + self.pe[:, :seq_len].requires_grad_(False)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, layer: nn.Module, embed_dim: int, p=0.1) -> None:\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.layer = layer\n",
        "        self.dropout = nn.Dropout(p=p)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.attn_weights = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        :param x: [N, seq_len, features]\n",
        "        :return: [N, seq_len, features]\n",
        "        \"\"\"\n",
        "        if isinstance(self.layer, nn.MultiheadAttention):\n",
        "            src = x.transpose(0, 1)     # [seq_len, N, features]\n",
        "            output, self.attn_weights = self.layer(src, src, src)\n",
        "            output = output.transpose(0, 1)     # [N, seq_len, features]\n",
        "\n",
        "        else:\n",
        "            output = self.layer(x)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "        output = self.norm(x + output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size: int) -> None:\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv1d(hidden_size, hidden_size * 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(hidden_size * 2, hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
        "        tensor = tensor.transpose(1, 2)\n",
        "        tensor = self.conv(tensor)\n",
        "        tensor = tensor.transpose(1, 2)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim: int, num_head: int, dropout_rate=0.1) -> None:\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attention = ResidualBlock(\n",
        "            nn.MultiheadAttention(embed_dim, num_head), embed_dim, p=dropout_rate\n",
        "        )\n",
        "        self.ffn = ResidualBlock(PositionWiseFeedForward(embed_dim), embed_dim, p=dropout_rate)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.attention(x)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DenseInterpolation(nn.Module):\n",
        "    def __init__(self, seq_len: int, factor: int) -> None:\n",
        "        \"\"\"\n",
        "        :param seq_len: sequence length\n",
        "        :param factor: factor M\n",
        "        \"\"\"\n",
        "        super(DenseInterpolation, self).__init__()\n",
        "\n",
        "        W = np.zeros((factor, seq_len), dtype=np.float32)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            s = np.array((factor * (t + 1)) / seq_len, dtype=np.float32)\n",
        "            for m in range(factor):\n",
        "                tmp = np.array(1 - (np.abs(s - (1+m)) / factor), dtype=np.float32)\n",
        "                w = np.power(tmp, 2, dtype=np.float32)\n",
        "                W[m, t] = w\n",
        "\n",
        "        W = torch.tensor(W).float().unsqueeze(0)\n",
        "        self.register_buffer(\"W\", W)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        w = self.W.repeat(x.shape[0], 1, 1).requires_grad_(False)\n",
        "        u = torch.bmm(w, x)\n",
        "        return u.transpose_(1, 2)\n",
        "\n",
        "\n",
        "class ClassificationModule(nn.Module):\n",
        "    def __init__(self, d_model: int, factor: int, num_class: int) -> None:\n",
        "        super(ClassificationModule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.factor = factor\n",
        "        self.num_class = num_class\n",
        "\n",
        "        self.fc = nn.Linear(int(d_model * factor), num_class)\n",
        "\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.contiguous().view(-1, int(self.factor * self.d_model))\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RegressionModule(nn.Module):\n",
        "    def __init__(self, d_model: int, factor: int, output_size: int) -> None:\n",
        "        super(RegressionModule, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.factor = factor\n",
        "        self.output_size = output_size\n",
        "        self.fc = nn.Linear(int(d_model * factor), output_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.contiguous().view(-1, int(self.factor * self.d_model))\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "E3LzXk6pqfDT"
      },
      "id": "E3LzXk6pqfDT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_29QElLsuvO"
      },
      "source": [
        ""
      ],
      "id": "s_29QElLsuvO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fle1d--8svm-"
      },
      "outputs": [],
      "source": [
        "class EncoderLayerForSAnD(nn.Module):\n",
        "    def __init__(self, input_features, seq_len, n_heads, n_layers, d_model: int = 128, dropout_rate=0.2) -> None:\n",
        "        super(EncoderLayerForSAnD, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.input_embedding = nn.Conv1d(input_features, d_model, 1)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, seq_len)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            EncoderBlock(d_model, n_heads, dropout_rate) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        x = x.type(torch.float)\n",
        "        x = self.input_embedding(x)\n",
        "\n",
        "        x = x.transpose(1, 2)\n",
        "        \n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        for l in self.blocks:\n",
        "            x = l(x)\\\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class SAnD(nn.Module):\n",
        "    \"\"\"\n",
        "    Simply Attend and Diagnose model\n",
        "\n",
        "    The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)\n",
        "\n",
        "    `Attend and Diagnose: Clinical Time Series Analysis Using Attention Models <https://arxiv.org/abs/1711.03905>`_\n",
        "    Huan Song, Deepta Rajan, Jayaraman J. Thiagarajan, Andreas Spanias\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self, input_features: int, seq_len: int, n_heads: int, factor: int,\n",
        "            n_class: int, n_layers: int, d_model: int = 128, dropout_rate: float = 0.2\n",
        "    ) -> None:\n",
        "        super(SAnD, self).__init__()\n",
        "        self.encoder = EncoderLayerForSAnD(input_features, seq_len, n_heads, n_layers, d_model, dropout_rate)\n",
        "        self.dense_interpolation = DenseInterpolation(seq_len, factor)\n",
        "        self.clf = ClassificationModule(d_model, factor, n_class)\n",
        "       \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \n",
        "        x = torch.LongTensor(x)\n",
        "        batch_size = x.shape[0]\n",
        "        # print(\"a\", x.shape)\n",
        "        x = self.encoder(x)\n",
        "        # print(\"b\", x.shape)\n",
        "        x = self.dense_interpolation(x)\n",
        "        # print(\"c\", x.shape)\n",
        "        x = self.clf(x)\n",
        "        # print(\"d\", x.shape)\n",
        "\n",
        "        probs = self.sigmoid(x)\n",
        "        # print(\"after sigmoid\", x.shape)\n",
        "\n",
        "        return probs.view((batch_size, num_class))\n",
        "\n",
        "\n",
        "in_feature = 39\n",
        "seq_len = 10\n",
        "n_heads = 32\n",
        "factor = 32\n",
        "num_class = 170\n",
        "num_layers = 6\n",
        "s_model = SAnD(in_feature, seq_len, n_heads, factor, num_class, num_layers)"
      ],
      "id": "Fle1d--8svm-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y38c32jhM9C"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(s_model.parameters(), lr=0.001)\n"
      ],
      "id": "6y38c32jhM9C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f22c2578"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "def eval_model(model, val_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    p_at_k = None\n",
        "    for x0, x1, y0, y1 in val_loader:\n",
        "        y_hat = model(x0)\n",
        "        y_score = torch.cat((y_score,  y_hat.detach().to(device)), dim=0)\n",
        "        y_hat = (y_hat > 0.5).int()\n",
        "        y_pred = torch.cat((y_pred, y_hat.detach().to(device)), dim=0)\n",
        "        y_true = torch.cat((y_true, y1.detach().to(device)), dim=0)\n",
        "        # for i in range(len(y_true)):\n",
        "        #   p_at_k = top_k_accuracy_score(y_true[i], y_score[i], k=5)\n",
        "      \n",
        "        #   print(p_at_k)\n",
        "    # print(y_true)\n",
        "        \n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
        "    return p, r, f"
      ],
      "id": "f22c2578"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdSyz8ffn1Qp"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "      model.train()\n",
        "      train_loss = 0\n",
        "      for x0, x1, y0, y1 in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x0)\n",
        "        loss = criterion(y_pred.squeeze(), y1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "      train_loss = train_loss / len(train_loader)\n",
        "      print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "      p, r, f = eval_model(model, val_loader)\n",
        "      print('Epoch: {} \\t Validation p: {:.4f}, r:{:.4f}, f: {:.4f}'\n",
        "              .format(epoch+1, p, r, f))"
      ],
      "id": "JdSyz8ffn1Qp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noG-tXtpn_I6",
        "outputId": "9d9353c5-d00b-4c4a-9d3e-1a77151c093f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \t Training Loss: 0.235545\n",
            "Epoch: 1 \t Validation p: 0.5615, r:0.2045, f: 0.2797\n",
            "Epoch: 2 \t Training Loss: 0.170191\n",
            "Epoch: 2 \t Validation p: 0.5550, r:0.2457, f: 0.3080\n",
            "Epoch: 3 \t Training Loss: 0.167281\n",
            "Epoch: 3 \t Validation p: 0.6416, r:0.2216, f: 0.3300\n",
            "Epoch: 4 \t Training Loss: 0.165589\n",
            "Epoch: 4 \t Validation p: 0.5279, r:0.2335, f: 0.3546\n",
            "Epoch: 5 \t Training Loss: 0.164378\n",
            "Epoch: 5 \t Validation p: 0.6244, r:0.1671, f: 0.2340\n",
            "Epoch: 6 \t Training Loss: 0.163228\n",
            "Epoch: 6 \t Validation p: 0.5682, r:0.2907, f: 0.3489\n",
            "Epoch: 7 \t Training Loss: 0.162273\n",
            "Epoch: 7 \t Validation p: 0.6050, r:0.1558, f: 0.2807\n",
            "Epoch: 8 \t Training Loss: 0.161062\n",
            "Epoch: 8 \t Validation p: 0.6006, r:0.1722, f: 0.2578\n",
            "Epoch: 9 \t Training Loss: 0.160461\n",
            "Epoch: 9 \t Validation p: 0.6039, r:0.2143, f: 0.3324\n",
            "Epoch: 10 \t Training Loss: 0.159784\n",
            "Epoch: 10 \t Validation p: 0.5402, r:0.2898, f: 0.3799\n",
            "Epoch: 11 \t Training Loss: 0.158937\n",
            "Epoch: 11 \t Validation p: 0.6575, r:0.2149, f: 0.3064\n",
            "Epoch: 12 \t Training Loss: 0.158024\n",
            "Epoch: 12 \t Validation p: 0.5918, r:0.2337, f: 0.3199\n",
            "Epoch: 13 \t Training Loss: 0.158003\n",
            "Epoch: 13 \t Validation p: 0.5512, r:0.2727, f: 0.3321\n",
            "Epoch: 14 \t Training Loss: 0.157141\n",
            "Epoch: 14 \t Validation p: 0.6331, r:0.1870, f: 0.3181\n",
            "Epoch: 15 \t Training Loss: 0.156425\n",
            "Epoch: 15 \t Validation p: 0.6236, r:0.1616, f: 0.2815\n",
            "Epoch: 16 \t Training Loss: 0.156032\n",
            "Epoch: 16 \t Validation p: 0.6373, r:0.2112, f: 0.2949\n",
            "Epoch: 17 \t Training Loss: 0.155393\n",
            "Epoch: 17 \t Validation p: 0.6103, r:0.2834, f: 0.3616\n",
            "Epoch: 18 \t Training Loss: 0.154711\n",
            "Epoch: 18 \t Validation p: 0.6105, r:0.2419, f: 0.3675\n",
            "Epoch: 19 \t Training Loss: 0.154342\n",
            "Epoch: 19 \t Validation p: 0.6027, r:0.2550, f: 0.3190\n",
            "Epoch: 20 \t Training Loss: 0.153816\n",
            "Epoch: 20 \t Validation p: 0.6112, r:0.1956, f: 0.3494\n",
            "Epoch: 21 \t Training Loss: 0.153503\n",
            "Epoch: 21 \t Validation p: 0.5908, r:0.2598, f: 0.3340\n",
            "Epoch: 22 \t Training Loss: 0.152795\n",
            "Epoch: 22 \t Validation p: 0.5513, r:0.2365, f: 0.3251\n",
            "Epoch: 23 \t Training Loss: 0.152552\n",
            "Epoch: 23 \t Validation p: 0.5773, r:0.1803, f: 0.3036\n",
            "Epoch: 24 \t Training Loss: 0.152005\n",
            "Epoch: 24 \t Validation p: 0.5965, r:0.2406, f: 0.3405\n",
            "Epoch: 25 \t Training Loss: 0.151417\n",
            "Epoch: 25 \t Validation p: 0.6018, r:0.2230, f: 0.3104\n",
            "Epoch: 26 \t Training Loss: 0.150918\n",
            "Epoch: 26 \t Validation p: 0.6116, r:0.2113, f: 0.2997\n",
            "Epoch: 27 \t Training Loss: 0.150362\n",
            "Epoch: 27 \t Validation p: 0.5716, r:0.2298, f: 0.3592\n",
            "Epoch: 28 \t Training Loss: 0.150097\n",
            "Epoch: 28 \t Validation p: 0.6051, r:0.2186, f: 0.2980\n",
            "Epoch: 29 \t Training Loss: 0.149369\n",
            "Epoch: 29 \t Validation p: 0.5775, r:0.2205, f: 0.3280\n",
            "Epoch: 30 \t Training Loss: 0.148924\n",
            "Epoch: 30 \t Validation p: 0.6016, r:0.1943, f: 0.2526\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 30\n",
        "train(s_model, train_loader, val_loader, n_epochs)"
      ],
      "id": "noG-tXtpn_I6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b75aG1sTyYeX"
      },
      "outputs": [],
      "source": [
        "\n"
      ],
      "id": "b75aG1sTyYeX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECLYzdxb-SYN"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ECLYzdxb-SYN"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Baseline SAnD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}