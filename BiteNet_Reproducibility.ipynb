{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jakqV3Scdyle"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "jakqV3Scdyle"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sladGIRtTaeg"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
      ],
      "id": "sladGIRtTaeg"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I5_clnk7T7yV"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "I5_clnk7T7yV"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9zgM6LTYIc1",
        "outputId": "8f36cf30-96bb-4585-e868-4a0fed64c317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/bitnet')\n"
      ],
      "id": "T9zgM6LTYIc1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JRzz_3FJYfz",
        "outputId": "c2e582b8-1957-4982-a010-5e99dc048924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if __name__=='__main__':\n",
        "    print('Using device:', device)"
      ],
      "id": "3JRzz_3FJYfz"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wMu3ZJFXY8Q8"
      },
      "outputs": [],
      "source": [
        "# !cat '/content/gdrive/My Drive/bitnet/processed/patients_mimic3_full.json'"
      ],
      "id": "wMu3ZJFXY8Q8"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9da94921"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import data_proc\n",
        "from data_proc import Dataset"
      ],
      "id": "9da94921"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fa4fb05",
        "outputId": "2bdd45ea-e771-4e7e-b893-4a3181962d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of second level category:  170\n",
            "Length of reverse dictionary  3875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7496"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset = Dataset()\n",
        "data = dataset.load_data()\n",
        "data = data\n",
        "len(data)"
      ],
      "id": "8fa4fb05"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86429275",
        "outputId": "1779326a-0c4f-4452-971a-9589e670b906"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39, 3875, 170)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "dataset.max_len_visit, dataset.vocabulary_size, dataset.digit3_size"
      ],
      "id": "86429275"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzLIHuH2LKLe",
        "outputId": "2b63f370-ae7b-40f0-d58d-352d9da32458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 31, 56, 61, 90, 105, 109, 114, 146]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(data[0][1])\n",
        "print(data[0][4])"
      ],
      "id": "VzLIHuH2LKLe"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuzba-RCk_3o",
        "outputId": "f18106ea-a96b-4e2e-b0b5-bf75c1e3aa8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_codes 3434\n"
          ]
        }
      ],
      "source": [
        "pids = [i[0] for i in data]\n",
        "intervals = [i[1] for i in data]\n",
        "seqs = [i[2] for i in data]\n",
        "\n",
        "\n",
        "readmission = [i[4] for i in data]\n",
        "diag = [i[3] for i in data]\n",
        "\n",
        "num_codes = set([code for visits in seqs for visit in visits for code in visit])\n",
        "num_codes = len(set(num_codes)) \n",
        "\n",
        "print(\"num_codes\",num_codes)\n",
        "\n",
        "\n",
        "assert len(pids) == len(seqs) == len(intervals) == len(readmission)"
      ],
      "id": "fuzba-RCk_3o"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSv5V_CKlOQt",
        "outputId": "a2c17daa-6862-476c-d64c-e5ec61e0b318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7496\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, seqs, intervals, readmission, diag):\n",
        "        self.seqs = seqs\n",
        "        self.intervals = intervals\n",
        "        self.y1 = readmission\n",
        "        self.y2 = diag\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.y1)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        return self.seqs[index], self.intervals[index], self.y1[index], self.y2[index]\n",
        "data = CustomDataset(seqs, intervals, readmission, diag)\n",
        "print(len(data))"
      ],
      "id": "uSv5V_CKlOQt"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyKvUqFqlb5U",
        "outputId": "c62ea173-bb18-4836-dd4e-6a57b33ffb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 6003\n",
            "Length of val dataset: 743\n",
            "Length of test dataset: 750\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "train_test_split = int(len(data)*0.9)\n",
        "lengths = [train_test_split, len(data) - train_test_split]\n",
        "train_data, test_data = random_split(data, lengths)\n",
        "\n",
        "\n",
        "train_val_split = int(len(train_data)*0.89)\n",
        "lengths = [train_val_split, len(train_data) - train_val_split]\n",
        "train_data, val_data = random_split(train_data, lengths)\n",
        "\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_data))\n",
        "print(\"Length of val dataset:\", len(val_data))\n",
        "print(\"Length of test dataset:\", len(test_data))\n"
      ],
      "id": "FyKvUqFqlb5U"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EKmgHLO_lzTm"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "  sequences, intervals, labels1, labels2 = zip(*data)\n",
        "\n",
        "  num_patients = len(sequences)\n",
        "  num_visits = len(sequences[0])\n",
        "  num_codes = len(sequences[0][0])\n",
        "\n",
        "\n",
        "\n",
        "  seq = torch.zeros((num_patients, num_visits, num_codes), dtype=torch.long)\n",
        "  masks = torch.zeros((num_patients, num_visits, num_codes), dtype=torch.bool)\n",
        "  y1 = torch.tensor(labels1, dtype=torch.float)\n",
        "  y2 = torch.tensor(labels2, dtype=torch.float)\n",
        "  itv = torch.zeros((num_patients, num_visits), dtype=torch.long)\n",
        "  \n",
        "  v_masks = torch.zeros((num_patients, num_visits), dtype=torch.bool)\n",
        "  \n",
        "\n",
        "  for i_patient, patient in enumerate(sequences):\n",
        "        for j_visit, visit in enumerate(patient):            \n",
        "            seq[i_patient][j_visit] = torch.tensor(sequences[i_patient][j_visit],dtype=torch.long)\n",
        "            for k in range(num_codes):\n",
        "              if seq[i_patient][j_visit][k] != 0:\n",
        "                masks[i_patient][j_visit][k] = True\n",
        "  for i_patient, patient in enumerate(intervals):\n",
        "        for j_visit, visit in enumerate(patient):            \n",
        "            itv[i_patient][j_visit] = torch.tensor(intervals[i_patient][j_visit],dtype=torch.long)\n",
        "            if itv[i_patient][j_visit] != 0:\n",
        "                v_masks[i_patient][j_visit]  = True\n",
        "  return sequences, intervals, masks, v_masks, y1, y2"
      ],
      "id": "EKmgHLO_lzTm"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N-gtAA4l2s8",
        "outputId": "dab4120d-596b-4550-da01-7f2741e9c71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3434\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(train_data, val_data, test_data, collate_fn):\n",
        "    \n",
        "    batch_size = 32\n",
        "    \n",
        "    train_loader = DataLoader(dataset = train_data, batch_size = 32, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(dataset = val_data, batch_size = 32, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(dataset = test_data, batch_size = 32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    \n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader = load_data(train_data, val_data, test_data, collate_fn)\n",
        "\n",
        "print(num_codes)"
      ],
      "id": "3N-gtAA4l2s8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_29QElLsuvO"
      },
      "source": [
        ""
      ],
      "id": "s_29QElLsuvO"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Fle1d--8svm-"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionPooling(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SelfAttentionPooling, self).__init__()\n",
        "        self.W = nn.Linear(input_dim, 1)\n",
        "    \n",
        "    def forward(self, batch_rep):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
        "      \n",
        "        attention_weight:\n",
        "            att_w : size (N, T, 1)\n",
        "    \n",
        "        return:\n",
        "            utter_rep: size (N, H)\n",
        "        \"\"\"\n",
        "        softmax = nn.functional.softmax\n",
        "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
        "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
        "\n",
        "        return utter_rep\n"
      ],
      "id": "Fle1d--8svm-"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-ijYt21ioAvY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention_O(nn.Module):\n",
        "    def __init__(self, direction, dropout, num_units, num_heads=10,  **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.direction = direction\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.num_units = num_units\n",
        "        self.q_linear = nn.LazyLinear(self.num_units, bias=False)\n",
        "        self.k_linear = nn.LazyLinear(self.num_units, bias=False)\n",
        "        self.v_linear = nn.LazyLinear(self.num_units, bias=False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # because of self-attention, queries and keys is equal to inputs\n",
        "        input_tensor, input_mask = inputs\n",
        "        # print(\"input_tensor.shape\", input_tensor.shape)\n",
        "        queries = input_tensor\n",
        "        keys = input_tensor\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.q_linear(queries)  # (N, L_q, d)\n",
        "        K = self.k_linear(keys)  # (N, L_k, d)\n",
        "        V = self.v_linear(keys)  # (N, L_k, d)\n",
        "\n",
        "        print('Q shape: ', Q.shape)\n",
        "\n",
        "        # Q shape:  torch.Size([320, 39, 100])\n",
        "        # outputs shape:  torch.Size([10, 39, 3200])\n",
        "        # input_mask:  torch.Size([320, 39])\n",
        "        # val_mask:  torch.Size([320, 39, 1])\n",
        "\n",
        "        # Split and concat\n",
        "        Q_ = torch.concat(torch.split(Q, self.num_heads, dim=2), axis=0)  # (h*N, L_q, d/h)\n",
        "        K_ = torch.concat(torch.split(K, self.num_heads, dim=2), axis=0)  # (h*N, L_k, d/h)\n",
        "        V_ = torch.concat(torch.split(V, self.num_heads, dim=2), axis=0)  # (h*N, L_k, d/h)\n",
        "\n",
        "        # Multiplication\n",
        "        outputs = torch.matmul(Q_, torch.transpose(K_, 2, 1))  # (h*N, L_q, L_k)\n",
        "\n",
        "        # print('outputs m shape: ', outputs.shape)\n",
        "\n",
        "        # Q shape:  torch.Size([320, \n",
        "\n",
        "        # Scale\n",
        "        outputs = outputs / (list(K_.shape)[-1] ** 0.5) # (h*N, L_q, L_k)\n",
        "\n",
        "        # print('outputs s shape: ', outputs.shape)/\n",
        "\n",
        "        # Key Masking\n",
        "        key_masks = torch.sign(torch.sum(torch.abs(K_), axis=-1))  # (h*N, T_k)\n",
        "        key_masks = torch.unsqueeze(key_masks, 1)  # (h*N, 1, T_k)\n",
        "        key_masks = torch.tile(key_masks, [1, list(Q_.shape)[1], 1])  # (h*N, T_q, T_k)\n",
        "\n",
        "        # Apply masks to outputs\n",
        "        paddings = torch.ones_like(outputs) * (-2 ** 32 + 1)  # exp mask\n",
        "        outputs = torch.where(key_masks == 0, paddings, outputs) # (h*N, T_q, T_k)\n",
        "        \n",
        "        # print('outputs masked shape: ', outputs.shape)\n",
        "\n",
        "\n",
        "        n_visits = input_tensor.shape[1]\n",
        "        sw_indices = torch.arange(0, n_visits, dtype=torch.int32)\n",
        "        sw_col, sw_row = torch.meshgrid(sw_indices, sw_indices)\n",
        "        # if self.direction == 'diag':\n",
        "        #     # shape of (n_visits, n_visits)\n",
        "        #     attention_mask = torch.t(tf.linalg.diag(- tf.ones([n_visits], tf.int32)) + 1, tf.bool)\n",
        "        # elif self.direction == 'forward':\n",
        "        #     attention_mask = tf.greater(sw_row, sw_col)  # shape of (n_visits, n_visits)\n",
        "        # else:\n",
        "        attention_mask = torch.greater(sw_col, sw_row)  # shape of (n_visits, n_visits)\n",
        "        adder = (1.0 - attention_mask.type(outputs.dtype)) * -10000.0\n",
        "\n",
        "        outputs = outputs + adder\n",
        "\n",
        "        # softmax\n",
        "        softmax = nn.Softmax(dim=-1)  # (h*N, T_q, T_k)\n",
        "        outputs = softmax(outputs)\n",
        "        # print('outputs softmax shape: ', outputs.shape)\n",
        "\n",
        "\n",
        "        # Query Masking\n",
        "        query_masks = torch.sign(torch.sum(torch.abs(Q_), axis=-1))  # (h*N, T_q)\n",
        "        query_masks = torch.unsqueeze(query_masks, -1)  # (h*N, T_q, 1)\n",
        "        query_masks = torch.tile(query_masks, [1, 1, K_.shape[1]])  # (h*N, T_q, T_k)\n",
        "\n",
        "        # print('query_masks shape: ', query_masks.shape)\n",
        "\n",
        "        # Apply masks to outputs\n",
        "        outputs = outputs * query_masks\n",
        "\n",
        "        # print('outputs masked shape: ', outputs.shape)\n",
        "\n",
        "        # Dropouts\n",
        "        outputs = self.dropout(outputs)\n",
        "        # Weighted sum\n",
        "        print('matmul: ', outputs.shape, V_.shape)\n",
        "\n",
        "        outputs = torch.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n",
        "        # print('outputs weighted sum shape: ', outputs.shape)\n",
        "\n",
        "\n",
        "        outputs = torch.split(outputs, self.num_heads, dim=0)\n",
        "        # print('outputs split shape: ', len(outputs), outputs[0].shape)\n",
        "\n",
        "        # Restore shape\n",
        "        outputs = torch.concat(outputs, axis=0)  # (N, L_q, d)\n",
        "\n",
        "        # print('outputs restore shape: ', outputs.shape)\n",
        "\n",
        "        # input padding\n",
        "        # print('outputs shape: ', outputs.shape)\n",
        "        # print('input_mask: ', input_mask.shape)\n",
        "        val_mask = torch.unsqueeze(input_mask, -1)\n",
        "        # print('val_mask: ', val_mask.shape)\n",
        "        outputs = torch.mul(outputs, val_mask.type(torch.float32))\n",
        "\n",
        "        return outputs"
      ],
      "id": "-ijYt21ioAvY"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ww-jfb01-aJh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        \n",
        "        batch_size, head, length, d_tensor = k.size()\n",
        "\n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.transpose(2, 3)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -e)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.softmax(score)\n",
        "\n",
        "        # 4. multiply with Value\n",
        "        v = score @ v\n",
        "\n",
        "        return v, score\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, n_head):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_concat = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, inputs, mask=None):\n",
        "        # 1. dot product with weight matrices\n",
        "        input_tensor, input_mask = inputs\n",
        "        # print(\"input_tensor.shape\", input_tensor.shape)\n",
        "        q = input_tensor\n",
        "        k = input_tensor\n",
        "        v = input_tensor\n",
        "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
        "\n",
        "        # 2. split tensor by number of heads\n",
        "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
        "\n",
        "        # 3. do scale dot product to compute similarity\n",
        "        out, attention = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "\n",
        "        # 5. visualize attention map\n",
        "        # TODO : we should implement visualization\n",
        "\n",
        "        return out\n",
        "\n",
        "    def split(self, tensor):\n",
        "        \"\"\"\n",
        "        split tensor by number of head\n",
        "        :param tensor: [batch_size, length, d_model]\n",
        "        :return: [batch_size, head, length, d_tensor]\n",
        "        \"\"\"\n",
        "        batch_size, length, d_model = tensor.size()\n",
        "\n",
        "\n",
        "        d_tensor = d_model // self.n_head\n",
        "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
        "        # it is similar with group convolution (split by number of heads)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def concat(self, tensor):\n",
        "        \"\"\"\n",
        "        inverse function of self.split(tensor : torch.Tensor)\n",
        "        :param tensor: [batch_size, head, length, d_tensor]\n",
        "        :return: [batch_size, length, d_model]\n",
        "        \"\"\"\n",
        "        batch_size, head, length, d_tensor = tensor.size()\n",
        "        d_model = head * d_tensor\n",
        "\n",
        "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
        "        return tensor"
      ],
      "id": "Ww-jfb01-aJh"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CBQ9UOGPudIj"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(nn.Module):\n",
        "  \"\"\"Fully connected feedforward network.\"\"\"\n",
        "\n",
        "  def __init__(self, hidden_size, filter_size, relu_dropout, allow_pad, **kwargs):\n",
        "    super(FeedForwardNetwork, self).__init__(**kwargs)\n",
        "    self.hidden_size = hidden_size\n",
        "    self.filter_size = filter_size\n",
        "    self.relu_dropout = relu_dropout\n",
        "    self.allow_pad = allow_pad\n",
        "\n",
        "    self.filter_dense_layer = nn.LazyLinear(filter_size)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.output_dense_layer = nn.LazyLinear(hidden_size)\n",
        "    self.dropout = nn.Dropout(p=self.relu_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"Return outputs of the feedforward network.\n",
        "\n",
        "    Args:\n",
        "      x: tensor with shape [batch_size, length, hidden_size]\n",
        "\n",
        "    Returns:\n",
        "      Output of the feedforward network.\n",
        "      tensor with shape [batch_size, length, hidden_size]\n",
        "    \"\"\"\n",
        "    x, mask = x\n",
        "    output = self.filter_dense_layer(x)\n",
        "    output = self.dropout(output)\n",
        "    output = self.output_dense_layer(output)\n",
        "\n",
        "    # input padding\n",
        "    # val_mask = tf.expand_dims(mask, -1)\n",
        "    # output = tf.multiply(output, tf.cast(val_mask, tf.float32))\n",
        "\n",
        "    return output"
      ],
      "id": "CBQ9UOGPudIj"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "b4PXt7v3mbgL"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    \"\"\"Transformer encoder stack.\n",
        "\n",
        "    The encoder stack is made up of N identical layers. Each layer is composed\n",
        "    of the sublayers:\n",
        "    1. Masked encoder layer\n",
        "    2. Feedforward network (which is 2 fully-connected layers)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, embedding_size, **kwargs):\n",
        "        super(EncoderStack, self).__init__(**kwargs)\n",
        "        self.layers = []\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_hidden_layers = params[\"num_hidden_layers\"]\n",
        "        for _ in range(params[\"num_hidden_layers\"]):\n",
        "            # Create sublayers for each layer.\n",
        "            # masked_encoder_layer = nn.MultiheadAttention(self.embedding_size, params[\"num_heads\"])\n",
        "            masked_encoder_layer = MultiHeadAttention(self.embedding_size,\n",
        "                                                      params[\"num_heads\"])\n",
        "            feed_forward_network = FeedForwardNetwork(params[\"hidden_size\"],\n",
        "                                                      params[\"filter_size\"],\n",
        "                                                      params[\"dropout\"],\n",
        "                                                      params[\"allow_ffn_pad\"])\n",
        "            self.layers.append([masked_encoder_layer, feed_forward_network])\n",
        "\n",
        "    def forward(self, inputs, input_mask):\n",
        "        encoder_inputs, input_mask = inputs, input_mask\n",
        "        \"\"\"Return the output of the encoder layer stacks.\n",
        "\n",
        "        Args:\n",
        "          encoder_inputs: tensor with shape [batch_size, number_visits, number_codes, hidden_size]\n",
        "          input_mask: mask for the encoder self-attention layer.\n",
        "            [batch_size, number_visits, number_codes]\n",
        "\n",
        "        Returns:\n",
        "          Output of encoder layer stack.\n",
        "          float32 tensor with shape [batch_size, inumber_visits, number_codes, hidden_size]\n",
        "        \"\"\"\n",
        "        for n, layer in enumerate(self.layers):\n",
        "          # Run inputs through the sublayers.\n",
        "          masked_encoder_layer = layer[0]\n",
        "          feed_forward_network = layer[1]\n",
        "\n",
        "          encoder_inputs = masked_encoder_layer((encoder_inputs, input_mask))\n",
        "          encoder_inputs = feed_forward_network((encoder_inputs, input_mask))\n",
        "\n",
        "        return encoder_inputs"
      ],
      "id": "b4PXt7v3mbgL"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "O0M2OU_LjHdL"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "from operator import mul\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def __init__(self, keep, **kwargs):\n",
        "        super(Flatten, self).__init__(**kwargs)\n",
        "        self.keep = keep\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        fixed_shape = list(inputs.shape)\n",
        "        start = len(fixed_shape) - self.keep\n",
        "        left = reduce(mul, [fixed_shape[i] or input.shape[i] for i in range(start)])\n",
        "        out_shape = [left] + [fixed_shape[i] or input.shape[i] for i in range(start, len(fixed_shape))]\n",
        "        flat = torch.reshape(inputs, out_shape)\n",
        "        return flat"
      ],
      "id": "O0M2OU_LjHdL"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "p4PJd1b6F0LL"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding1D, self).__init__()\n",
        "        self.org_channels = channels\n",
        "        channels = int(np.ceil(channels / 2) * 2)\n",
        "        self.channels = channels\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.cached_penc = None\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        \"\"\"\n",
        "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
        "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
        "        \"\"\"\n",
        "        if len(tensor.shape) != 3:\n",
        "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "\n",
        "        if self.cached_penc is not None and self.cached_penc.shape == tensor.shape:\n",
        "            return self.cached_penc\n",
        "\n",
        "        self.cached_penc = None\n",
        "        batch_size, x, orig_ch = tensor.shape\n",
        "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
        "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1)\n",
        "        emb = torch.zeros((x, self.channels), device=tensor.device).type(tensor.type())\n",
        "        emb[:, : self.channels] = emb_x\n",
        "\n",
        "        self.cached_penc = emb[None, :, :orig_ch].repeat(batch_size, 1, 1)\n",
        "        return self.cached_penc\n",
        "        \n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, seq_len) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "\n",
        "        for pos in range(seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1)) / d_model)))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x) -> torch.Tensor:\n",
        "        seq_len = x.shape[1]\n",
        "        x = math.sqrt(self.d_model) * x\n",
        "        x = x + self.pe[:, :seq_len].requires_grad_(False)\n",
        "        return x"
      ],
      "id": "p4PJd1b6F0LL"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10ca7480",
        "outputId": "c4403a91-b81e-450c-8dc3-d0062ddf860b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "class BiteNet(nn.Module):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super(BiteNet, self).__init__()\n",
        "        self.device = device\n",
        "        self.lr = 0.001\n",
        "        self.dropout_rate = 0.2\n",
        "        self.n_intervals = 12 * 365 + 1\n",
        "        self.n_visits = 10\n",
        "        self.n_codes = dataset.max_len_visit\n",
        "        self.vocabulary_size = dataset.vocabulary_size\n",
        "        self.digit3_size = dataset.digit3_size\n",
        "        self.embedding_size = 100\n",
        "        self.num_hidden_layers = 5\n",
        "        self.predict_type = \"re\"\n",
        "        self.batch_size = 32\n",
        "        self.num_heads = 10\n",
        "        \n",
        "        self.params = dict()\n",
        "        self.params[\"hidden_size\"] = self.embedding_size\n",
        "        self.params[\"filter_size\"] = self.embedding_size\n",
        "        self.params[\"dropout\"] = self.dropout_rate\n",
        "        self.params[\"allow_ffn_pad\"] = False\n",
        "        self.params[\"num_hidden_layers\"] = self.num_hidden_layers\n",
        "        self.params[\"is_scale\"] = False\n",
        "        self.params[\"direction\"] = 'diag'\n",
        "        self.params[\"num_heads\"] = self.num_heads\n",
        "\n",
        "        \n",
        "        self.flatten_1 = Flatten(1)\n",
        "        self.flatten_2 = Flatten(2)\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
        "        self.interval_embedding = nn.Embedding(self.n_intervals, self.embedding_size)\n",
        "        self.encoder_stack = EncoderStack(self.params, self.embedding_size)\n",
        "        self.attention_pooling = SelfAttentionPooling(self.embedding_size)\n",
        "        self.fc1 = nn.LazyLinear(self.embedding_size)\n",
        "        self.fc2 = nn.LazyLinear(1)\n",
        "        self.fc3 = nn.LazyLinear(170)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
        "        self.position_encoding = PositionalEncoding1D(self.embedding_size)\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src == 0 # padding idx\n",
        "        # return src_mask\n",
        "        return src_mask.to(self.device) # (batch_size, max_src_len)\n",
        "\n",
        "  \n",
        "    def forward(self, x0, x1, masks, v_masks):\n",
        "        seq, intervals = torch.tensor(x0), torch.tensor(x1)\n",
        "        # print(\"seq shape\", seq.shape)\n",
        "        # print(\"interval shape\", intervals.shape)\n",
        "\n",
        "        batch_size = seq.shape[0]\n",
        "        src_emb = self.embedding(seq)\n",
        "        e = self.flatten_2(src_emb)\n",
        "        # print(\"e.shape\", e.shape)\n",
        "        # input mask, reshape 3 dimension to 2\n",
        "        e_mask = self.flatten_1(masks)\n",
        "        # print(\"e_mask.shape\", e_mask.shape) \n",
        "\n",
        "        # Vanilla Encoder\n",
        "        h = self.encoder_stack(e, e_mask)\n",
        "        # Attention pooling\n",
        "        # print(\"before pooling\", h.shape)\n",
        "        v = self.attention_pooling(h)\n",
        "        # print(\"after pooling\", v.shape)\n",
        "        \n",
        "        e_p = self.interval_embedding(intervals)\n",
        "        e_p = self.flatten_1(e_p)\n",
        "        \n",
        "\n",
        "        v = v + e_p\n",
        "        # print(\"v\", v.shape)\n",
        "        v = v.view(batch_size, -1, self.embedding_size)\n",
        "        \n",
        "        h2 = self.encoder_stack(v, v_masks)\n",
        "        # print(\"before pooling 2\", h2.shape)\n",
        "        out = self.attention_pooling(h2)\n",
        "        # print(\"after pooling\", out.shape)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        # print(\"after relu\", out.shape)\n",
        "\n",
        "        \n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "        \n",
        "        probs = self.sigmoid(out)\n",
        "        # print(\"after sigmoid\", probs.shape)\n",
        "        return probs\n",
        "        return probs.view((batch_size, 170))\n",
        "bite_net = BiteNet(dataset=dataset)"
      ],
      "id": "10ca7480"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6y38c32jhM9C"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(bite_net.parameters(), lr=0.001)\n"
      ],
      "id": "6y38c32jhM9C"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f22c2578"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, precision_recall_curve, auc\n",
        "def eval_model(model, val_loader):\n",
        "    \n",
        "    model.eval()\n",
        "    y_pred = torch.LongTensor()\n",
        "    y_score = torch.Tensor()\n",
        "    y_true = torch.LongTensor()\n",
        "    model.eval()\n",
        "    for x0, x1, masks, v_masks, y0, y1 in val_loader:\n",
        "        y_hat = model(x0, x1, masks, v_masks)\n",
        "        # print(y)\n",
        "        # print(y_hat)\n",
        "        \n",
        "        # print(\"y_hat\", y_hat)\n",
        "        y_score = torch.cat((y_score,  y_hat.detach().to(device)), dim=0)\n",
        "        y_hat = (y_hat > 0.5).int()\n",
        "        y_pred = torch.cat((y_pred, y_hat.detach().to(device)), dim=0)\n",
        "        y_true = torch.cat((y_true, y1.detach().to(device)), dim=0)\n",
        "        \n",
        "    torch.set_printoptions(profile=\"full\")\n",
        "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
        "    # roc_auc = roc_auc_score(y_true, y_score, average='micro')\n",
        "\n",
        "    # precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
        "    # Use AUC function to calculate the area under the curve of precision recall curve\n",
        "    # auc_precision_recall = auc(recall, precision)\n",
        "    # auc_precision_recall = 0\n",
        "    return p, r, f"
      ],
      "id": "f22c2578"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JdSyz8ffn1Qp"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, n_epochs):\n",
        "    for epoch in range(n_epochs):\n",
        "      model.train()\n",
        "      train_loss = 0\n",
        "      for x0, x1, masks, v_masks, y0, y1 in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(x0, x1, masks, v_masks)\n",
        "\n",
        "        loss = criterion(y_pred.squeeze(), y1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "      train_loss = train_loss / len(train_loader)\n",
        "      print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
        "      p, r, f = eval_model(model, val_loader)\n",
        "      print('Epoch: {} \\t Validation p: {:.4f}, r:{:.4f}, f: {:.4f}'\n",
        "              .format(epoch+1, p, r, f))\n",
        "      "
      ],
      "id": "JdSyz8ffn1Qp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noG-tXtpn_I6",
        "outputId": "e3509dd6-d5ba-48ea-9662-fc86504f6d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 10\n",
        "train(bite_net, train_loader, val_loader, n_epochs)"
      ],
      "id": "noG-tXtpn_I6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rscf3uzFFyz6"
      },
      "outputs": [],
      "source": [
        "   "
      ],
      "id": "Rscf3uzFFyz6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcHY2OZjonmA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\""
      ],
      "id": "LcHY2OZjonmA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUxkayyN-XyC"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ZUxkayyN-XyC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b75aG1sTyYeX"
      },
      "outputs": [],
      "source": [
        "\n"
      ],
      "id": "b75aG1sTyYeX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECLYzdxb-SYN"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ECLYzdxb-SYN"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BiteNet_Reproducibility.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}