{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZtKsbcbCEp-"},"outputs":[],"source":["import sys,os\n","import random\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from google.colab import drive, files\n","import pickle as pickle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":618,"status":"ok","timestamp":1651909260977,"user":{"displayName":"Jacky S","userId":"12056070040940406337"},"user_tz":240},"id":"sq22Y808CHTX","outputId":"23196002-84dd-4d97-e16b-5eedc18703cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["drive.mount('/content/drive')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1651909260978,"user":{"displayName":"Jacky S","userId":"12056070040940406337"},"user_tz":240},"id":"KREoVhHIV-TH","outputId":"bf6e1f06-8ab9-420b-f5b1-202a7515ab96"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","if __name__=='__main__':\n","    print('Using device:', device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4yZZHTckD6GR"},"outputs":[],"source":["DATA_PATH = '/content/drive/My Drive/BiteNetProject/data_processing/'\n","\n","data = pickle.load(open(os.path.join(DATA_PATH,'data.pkl'), 'rb'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kw6kNyPVbu5F"},"outputs":[],"source":["## MANIPULATE AND PROCESS FEATURES\n","## Seqs: Get list of patient visits, where each visit is a list of medical codes \n","## [patient_1[visit1...visit10]....patient_2[visit1....visit10]], each visit is a list [medcode1....medcode10]\n","seqs= [i[2] for i in data]\n","\n","time_interval = [i[1] for i in data]\n","\n","\n","\n","#Subtract 1 from time interval and make it sequential difference in months between visits\n","for patient_i, patient in enumerate(time_interval):\n","    time_interval[patient_i] = [(patient[visit_j] - patient[visit_j - 1])//30 for visit_j, visit in enumerate(patient) if visit_j>0]\n","\n","for patient_i, patient in enumerate(time_interval):\n","    for visit_j, visit in enumerate(patient):\n","      if visit <= 1:\n","        patient[visit_j] = \"0-1m\" \n","      if visit > 1 and visit <= 3:\n","        patient[visit_j] = \"1-3m\" \n","      if visit > 3 and visit <= 6:\n","        patient[visit_j] = \"1-6m\" \n","      if visit > 6 and visit <= 12:\n","        patient[visit_j] = \"6-12m\"\n","      if visit > 12:\n","        patient[visit_j] = \"12+m\"\n","\n","\n","## remove 0s from visits and patients // dont need padding for the CNN\n","\n","for patient_i, patient in enumerate(seqs):\n","  seqs[patient_i] = [visit for visit in patient if sum(visit)>0]\n","\n","for patient_i, patient in enumerate(seqs):\n","  for visit_j, visit in enumerate(patient):\n","    patient[visit_j] = [str(medcode) for medcode in visit if (medcode > 0)]\n","    random.shuffle(patient[visit_j])\n","\n","\n","\n","## Append time interval to the end\n","for patient_i, patient in enumerate(seqs):\n","  for visit_j, visit in enumerate(patient):\n","    if visit_j <= 8:\n","      patient[visit_j].append(time_interval[patient_i][visit_j])\n","\n","\n","\n","## Flatten the list of visits so each patient has a list of medcodes \n","\n","for patient_i, patient in enumerate(seqs):\n","  seqs[patient_i] = [medcode for visit in patient for medcode in visit][:100]\n","## Feature \n","\n","\n","\n","## Num of Codes\n","unique_codes = (set([code for patient in seqs for code in patient]))\n","\n","num_codes = len(unique_codes)\n","\n","\n","## target label of readmission \n","diag = [i[3] for i in data]\n","\n","\n","\n","assert len(seqs) == len(diag)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtplGsFob_Ic"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, seqs, diag):\n","        \n","        self.x = self.convert2idx(seqs)\n","        self.y = diag\n","    \n","    def convert2idx(self,seqs):\n","        word2idx = {}\n","        j = 0\n","\n","        for patientid, patient in enumerate(seqs):\n","          for visitid, visit in enumerate(patient):\n","            if visit in word2idx:\n","              patient[visitid] = word2idx[visit]\n","            if visit not in word2idx:\n","              word2idx[visit] = j\n","              patient[visitid] = j\n","              j+=1\n","        return seqs\n","\n","\n","    def __len__(self):\n","        \n","        return len(self.x)\n","    \n","    def __getitem__(self, index):\n","\n","        return self.x[index],self.y[index]\n","        \n","data = CustomDataset(seqs, diag)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651909261651,"user":{"displayName":"Jacky S","userId":"12056070040940406337"},"user_tz":240},"id":"QFvEmuyYeFMP","outputId":"f34d6e43-e4a4-4f76-ebdb-be83c6519abd"},"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataset.Subset object at 0x7f9ac0916150>\n","Length of train dataset: 2998\n","Length of val dataset: 2998\n","Length of test dataset: 1500\n"]}],"source":["from torch.utils.data.dataset import random_split\n","\n","train_test_split = int(len(data)*0.8)\n","lengths = [train_test_split, len(data) - train_test_split]\n","train_data, test_data = random_split(data, lengths)\n","\n","\n","train_val_split = int(len(train_data)*0.5)\n","lengths = [train_val_split, len(train_data) - train_val_split]\n","train_data, val_data = random_split(train_data, lengths)\n","\n","print(train_data)\n","print(\"Length of train dataset:\", len(train_data))\n","print(\"Length of val dataset:\", len(val_data))\n","print(\"Length of test dataset:\", len(test_data))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aAUx16Lthy9R"},"outputs":[],"source":["def collate_fn(data):\n","  sequences, labels = zip(*data)\n","\n","  num_patients = len(sequences)\n","\n","  x = torch.zeros((num_patients, 100), dtype=torch.long)\n","  \n","  y = torch.tensor(labels, dtype=torch.float)\n","\n","  for ipatient, patient in enumerate(sequences):\n","    padding_needed = 100 - len(patient) \n","    document = sequences[ipatient] + [0] * padding_needed       \n","    x[ipatient] = torch.tensor(document,dtype=torch.long)\n","\n","\n","  return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQjznChpczkL"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","def load_data(train_data, val_data, test_data, collate_fn):\n","    \n","    batch_size = 32\n","    ## iter will get a batch of size 32 [10 visits x 39 codes ] \n","\n","    train_loader = DataLoader(dataset = train_data, batch_size = 32, shuffle=True, collate_fn=collate_fn)\n","    val_loader = DataLoader(dataset = val_data, batch_size = 32, shuffle=False, collate_fn=collate_fn)\n","    test_loader = DataLoader(dataset = test_data, batch_size = 32, shuffle=False, collate_fn=collate_fn)\n","\n","    \n","    return train_loader, val_loader, test_loader\n","\n","\n","train_loader, val_loader, test_loader = load_data(train_data, val_data, test_data, collate_fn)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1651909261792,"user":{"displayName":"Jacky S","userId":"12056070040940406337"},"user_tz":240},"id":"BAVjfDbEeC4u","outputId":"1cccbe99-178b-4e99-cb0e-1b5c1738d167"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (embedding): Embedding(3425, 100)\n","  (conv1): Conv2d(1, 1, kernel_size=(3, 1), stride=(1, 1))\n","  (relu): ReLU()\n","  (maxpool): MaxPool2d(kernel_size=(98, 1), stride=(98, 1), padding=0, dilation=1, ceil_mode=False)\n","  (linear): Linear(in_features=100, out_features=170, bias=True)\n","  (sigmoid): Sigmoid()\n",")"]},"metadata":{},"execution_count":21}],"source":["class CNN(nn.Module):\n","    \n","    def __init__(self, num_codes, embedding_dim=100):\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(num_codes, embedding_dim)\n","   \n","        self.conv1 = nn.Conv2d(in_channels = 1, kernel_size = (3,1), out_channels = 1) ## in_channels = 1, select 3 filters so 3 out_channels\n","\n","        self.relu = nn.ReLU()\n","\n","        self.maxpool = nn.MaxPool2d(kernel_size=(98,1))\n","\n","\n","        self.linear = nn.Linear(100,170)\n","\n","        self.sigmoid = nn.Sigmoid()\n","    \n","    def forward(self, x):\n","        #print(x.shape) #32 100\n","        batch_size = x.shape[0]\n","        x = self.embedding(x).unsqueeze(1)\n","        #print(\"after embd\",x.shape) #[32, 1, 100, 100]\n","\n","        x = self.conv1(x).squeeze(1)\n","\n","        #print(\"after conv\",x.shape)\n","\n","        x = self.relu(x)\n","\n","        #print(x.shape)\n","\n","        max_x = self.maxpool(x).squeeze(1)\n","\n","        #print(\"after max pool\",max_x.shape)\n","\n","        result = self.linear(max_x)\n","\n","        result = self.sigmoid(result)\n","\n","        #print(result)\n","\n","\n","        return result\n","\n","\n","# load the model here\n","MODEL = CNN(num_codes = num_codes)\n","MODEL\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbyU7oUcpm7E"},"outputs":[],"source":["import torch.optim as optim\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(MODEL.parameters(), lr=0.001)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L35GTsBzp4gI"},"outputs":[],"source":["from sklearn.utils.validation import indexable\n","from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import precision_recall_curve, auc\n","from sklearn.metrics import precision_score\n","\n","def eval_model(model, loader):\n","    model.eval()\n","    y_pred = torch.LongTensor()\n","    y_true = torch.LongTensor()\n","    model.eval()\n","    for x, y in loader:\n","        y_hat = model(x)\n","        y_hat = y_hat > 0.5\n","        y_hat = y_hat.int()\n","        y = y.int()\n","        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n","        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n","    \n","    precision = precision_score(y_true,y_pred,average=\"micro\")\n","\n","\n","    return precision"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjO7i1LHrQiH"},"outputs":[],"source":["def train(model, train_loader, val_loader, n_epochs, print_train_results=True):\n","    for epoch in range(n_epochs):\n","      model.train()\n","      train_loss = 0\n","      for x, y in train_loader:\n","        loss = None\n","        optimizer.zero_grad()\n","        y_hat = model(x)\n","\n","        loss = criterion(y_hat,y)\n","        loss.backward()\n","        optimizer.step()\n","        # your code here\n","        \n","        train_loss += loss.item()\n","      train_loss = train_loss / len(train_loader)\n","      if print_train_results==True:\n","        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n","      precision = eval_model(model, val_loader)\n","      if print_train_results==True:\n","        print('Epoch: {} \\t Validation overall precision p:{:.3f}'.format(epoch+1,precision))\n","      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TALVyIpxr4D0"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5Ldkp6lr4HS","executionInfo":{"status":"ok","timestamp":1651909293653,"user_tz":240,"elapsed":31574,"user":{"displayName":"Jacky S","userId":"12056070040940406337"}},"outputId":"1d570a8f-a4d0-4580-dd4e-db0a51b99302"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 1 \t Training Loss: 0.297297\n","Epoch: 1 \t Validation overall precision p:0.593\n","Epoch: 2 \t Training Loss: 0.164346\n","Epoch: 2 \t Validation overall precision p:0.620\n","Epoch: 3 \t Training Loss: 0.158474\n","Epoch: 3 \t Validation overall precision p:0.633\n","Epoch: 4 \t Training Loss: 0.155890\n","Epoch: 4 \t Validation overall precision p:0.636\n","Epoch: 5 \t Training Loss: 0.154096\n","Epoch: 5 \t Validation overall precision p:0.651\n","Epoch: 6 \t Training Loss: 0.152572\n","Epoch: 6 \t Validation overall precision p:0.648\n","Epoch: 7 \t Training Loss: 0.151190\n","Epoch: 7 \t Validation overall precision p:0.660\n","Epoch: 8 \t Training Loss: 0.149960\n","Epoch: 8 \t Validation overall precision p:0.683\n","Epoch: 9 \t Training Loss: 0.148780\n","Epoch: 9 \t Validation overall precision p:0.672\n","Epoch: 10 \t Training Loss: 0.147624\n","Epoch: 10 \t Validation overall precision p:0.690\n"]}],"source":["n_epochs = 10\n","train(MODEL, train_loader, val_loader, n_epochs)"]},{"cell_type":"code","source":["def test(model, test_loader, test_number):\n","      precision = eval_model(model, test_loader)\n","      \n","      \n","      print('Test: test_number{} \\t Test precision :{:.3f}'\n","              .format(test_number+1,precision))"],"metadata":{"id":"w9wDOWgsTTv-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_number = 3\n","for i in range(test_number):\n","  train_test_split = int(len(data)*0.8)\n","  lengths = [train_test_split, len(data) - train_test_split]\n","  train_data, test_data = random_split(data, lengths)\n","\n","\n","  train_val_split = int(len(train_data)*0.5)\n","  lengths = [train_val_split, len(train_data) - train_val_split]\n","  train_data, val_data = random_split(train_data, lengths)\n","  \n","  train_loader, val_loader, test_loader = load_data(train_data, val_data, test_data, collate_fn)\n","  \n","  newmodel = CNN(num_codes = num_codes)\n","  criterion = nn.BCELoss()\n","  optimizer = optim.Adam(newmodel.parameters(), lr=0.001)\n","\n","  n_epochs = 10\n","  train(newmodel, train_loader, val_loader, n_epochs,print_train_results=False)\n","  test(newmodel, test_loader, i)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pODFfstiFf6","executionInfo":{"status":"ok","timestamp":1651909856238,"user_tz":240,"elapsed":83173,"user":{"displayName":"Jacky S","userId":"12056070040940406337"}},"outputId":"b8bccba8-29ed-43b5-cd0f-2ac9f5a87fed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test: test_number1 \t Test precision :0.690\n","Test: test_number2 \t Test precision :0.666\n","Test: test_number3 \t Test precision :0.655\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Baseline Deepr diagnosis prediction.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}